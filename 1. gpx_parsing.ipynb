{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Parsing Strava Datasets\n",
    "\n",
    "This notebook will parse all Strava GPX files and construct the necessary database.\n",
    "\n",
    "## Step 0:  Global Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = 'bike_data.db'\n",
    "epsg_code = 32613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import geopy.distance\n",
    "import datetime\n",
    "from math import sqrt, floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import sqlite3 as sql\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from pyproj import CRS, Proj, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove the database and re-initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing bike_data.db\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists( database_path ):\n",
    "    print('Removing {}'.format( database_path ))\n",
    "    os.remove( database_path )\n",
    "    \n",
    "conn = sql.connect( database_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTM Zone: 13\n"
     ]
    }
   ],
   "source": [
    "#  Setup the Projection Transformer\n",
    "crs = CRS.from_epsg( epsg_code )\n",
    "proj = Transformer.from_crs(crs.geodetic_crs, crs)\n",
    "utm_zone = int(re.findall(\"\\d+\", crs.utm_zone)[0])\n",
    "print('UTM Zone: {}'.format(utm_zone))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Tables\n",
    "\n",
    "#### sector_list\n",
    "\n",
    "This table contains the list of sectors, the number of points in each, and it's name.\n",
    "\n",
    "| index          | sector_name  | sector_id    |  number_points |\n",
    "| :------------- | :----------- | :----------- | :------------- |\n",
    "|  Integer       | String       | String       | Integer        |\n",
    "|  0             | Sector 1     | sector_1     | 14             |\n",
    "\n",
    "* Sector Name: \n",
    "  * Descriptive name of the sector.\n",
    "* Sector-ID: \n",
    "  * Table that contains the point data.\n",
    "* Number-Points:\n",
    "  * Number of points in the table for the sector polygon.\n",
    "\n",
    "#### sector_X\n",
    "\n",
    "This table breaks down an individual sector.\n",
    "\n",
    "| index   | latitude    | longitude   | elevation  |\n",
    "| :------ | :---------- | :---------- | :--------- |\n",
    "| Integer | Float       | Float       | Float      |\n",
    "| 0       | 38.12345    | -104.1243   | 1713       |\n",
    "\n",
    "#### point_list\n",
    "\n",
    "This table contains the full list of points collected from GPX files.\n",
    "\n",
    "| index   | longitude | latitude  | elevation | timestamp                 | step_dist    | time_diff_sec | sector_id | dataset                      | dataset_id |\n",
    "| :------ | :-------- | :-------- | :-------- | :------------------------ | :----------- | :------------ | :-------- | :--------------------------- | :--------- |\n",
    "| Integer | Float     | Float     | Float     | String                    | Float        | Float         | String    | String                       | Integer    |\n",
    "|   3     | 39.12345  | -104.1234 | 1713.123  | 2020-11-20 00:03:18+00:00 | 3.7842052780 | 1             | sector_1  | ./datasets/ride.20201120.gpx | 1          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Load the Sector Polygons\n",
    "\n",
    "In order to classify each track point, we need to assign it to a sector.  The sector KML file has each sector in the form of a KML polygon.  This block will load the sector KML file and load the sectors into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sector map\n",
    "import KML_Parser\n",
    "kml_file = KML_Parser.Bike_Sector_KML_File()\n",
    "bike_sector_polygons = kml_file.Parse_KML( 'bike_sectors.kml' )\n",
    "\n",
    "#  Create a table list\n",
    "table_list = ['sector_' + str(x) for x in range(0, len(bike_sector_polygons))]\n",
    "dataset = { 'sector_name'  : [ x['name'] for x in bike_sector_polygons ],\n",
    "            'sector_id'    : table_list,\n",
    "            'number_points': [ len(x['polygon']) for x in bike_sector_polygons ] }\n",
    "pd.DataFrame( data = dataset ).to_sql( 'sector_list', conn )\n",
    "\n",
    "#  Create a table for each sector\n",
    "counter = 0\n",
    "for sector in bike_sector_polygons:\n",
    "    table_name = table_list[counter]\n",
    "    dataset = { 'latitude':  [ x[1] for x in sector['polygon'] ],\n",
    "                'longitude': [ x[0] for x in sector['polygon'] ],\n",
    "                'gridZone':  [ utm_zone for x in sector['polygon'] ],\n",
    "                'isNorth':   [ True for x in sector['polygon'] ],\n",
    "                'easting':   [ 0    for x in sector['polygon'] ],\n",
    "                'northing':  [ 0    for x in sector['polygon'] ],\n",
    "                'elevation': [ x[2] for x in sector['polygon'] ] }\n",
    "    \n",
    "    #  Compute UTM Coordinates\n",
    "    for x in range( 0, len(sector['polygon'])):\n",
    "        (easting, northing) = proj.transform( sector['polygon'][x][1],\n",
    "                                              sector['polygon'][x][0] )\n",
    "        dataset['easting'][x]  = easting\n",
    "        dataset['northing'][x] = northing\n",
    "    \n",
    "    pd.DataFrame( data = dataset ).to_sql( table_name, conn )\n",
    "    \n",
    "    #  Create Shapely Polygon to Aid Point-in-Polygon Searches\n",
    "    bike_sector_polygons[counter]['shape']     = Polygon( [ (x[0], x[1]) for x in sector['polygon'] ] )\n",
    "    bike_sector_polygons[counter]['sector_id'] = table_name\n",
    "    \n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map points to sectors, we need a lookup method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Sector( sector_polygon_list, point ):\n",
    "    \n",
    "    #  Iterate over each polygon\n",
    "    for x in range( 0, len( sector_polygon_list ) ):\n",
    "        if sector_polygon_list[x]['shape'].contains( point ):\n",
    "            return sector_polygon_list[x]['sector_id']\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 4: Load the GPX Data and write to the database\n",
    "\n",
    "This is a multi-step process.\n",
    "* Load each GPX Dataset\n",
    "* For each dataset\n",
    "  * Assign a sector\n",
    "* Write all points to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./datasets/ride.20201120.gpx, Dataset-ID: 0\n",
      "Loading: ./datasets/ride.20201123.gpx, Dataset-ID: 1\n",
      "Database Written to Disk\n"
     ]
    }
   ],
   "source": [
    "#  Look for GPX Files\n",
    "dataset_id = 0\n",
    "df = pd.DataFrame( columns=['longitude',\n",
    "                            'latitude',\n",
    "                            'gridZone',\n",
    "                            'easting',\n",
    "                            'northing',\n",
    "                            'elevation',\n",
    "                            'timestamp', \n",
    "                            'stepDist',\n",
    "                            'timeDiffSec',\n",
    "                            'sectorId',\n",
    "                            'dataset',\n",
    "                            'datasetId'])\n",
    "for root, dirs, files in os.walk( \"./datasets\", topdown=False ):\n",
    "    for name in files:\n",
    "        fname = os.path.join( root, name )\n",
    "        if os.path.splitext( fname )[-1] == '.gpx':\n",
    "            print('Loading: {}, Dataset-ID: {}'.format( fname, dataset_id ))\n",
    "            gpx_file = open( fname )\n",
    "            gpx_data = gpxpy.parse( gpx_file )\n",
    "            \n",
    "            point_data = gpx_data.tracks[0].segments[0].points\n",
    "            \n",
    "            first_point = True\n",
    "            prev_point = None\n",
    "            for point in point_data:\n",
    "                \n",
    "                #  Assign a sector-id to the point\n",
    "                sector_id = Find_Sector( bike_sector_polygons, Point([point.longitude, point.latitude]) )\n",
    "                time_diff = 0\n",
    "                distance_prev = 0\n",
    "                if first_point:\n",
    "                    first_point = False\n",
    "                else:\n",
    "                    distance_prev = geopy.distance.geodesic( (prev_point.latitude, prev_point.longitude),\n",
    "                                                             (     point.latitude,      point.longitude) ).m\n",
    "                    time_diff = ( point.time - prev_point.time).total_seconds()\n",
    "                    \n",
    "                (easting,northing) = proj.transform( point.latitude, point.longitude )\n",
    "                #print( 'Point: {}, {}, Easting: {}, Northing: {}'.format( point.latitude, \n",
    "                #                                                          point.longitude,\n",
    "                #                                                          easting,\n",
    "                #                                                          northing))\n",
    "                    \n",
    "                df = df.append({'longitude'   : point.longitude, \n",
    "                                'latitude'    : point.latitude,\n",
    "                                'gridZone'    : utm_zone,\n",
    "                                'easting'     : easting,\n",
    "                                'northing'    : northing,\n",
    "                                'elevation'   : point.elevation,\n",
    "                                'timestamp'   : str(point.time),\n",
    "                                'stepDist'    : distance_prev,\n",
    "                                'timeDiffSec' : time_diff,\n",
    "                                'sectorId'    : sector_id,\n",
    "                                'dataset'     : fname,\n",
    "                                'datasetId'   : int(dataset_id) }, ignore_index=True )\n",
    "                prev_point = point\n",
    "                \n",
    "            dataset_id += 1\n",
    "\n",
    "df\n",
    "df.to_sql( 'point_list', conn )\n",
    "print('Database Written to Disk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
