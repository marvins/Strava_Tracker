{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Track Analysis\n",
    "\n",
    "In order to properly analyze our GPX tracks, we need to put all of them into the same _frame-of-reference_.   This is accomplished by the following algorithm...\n",
    "\n",
    "* Create a set of variables with the initialized values\n",
    "    * Current Coordinate = Starting Coordinate\n",
    "        * $P_{\\textrm{cur}} = P_{\\textrm{start}}$\n",
    "        * **NOTE:** Starting coordinate $P_{\\textrm{start}}$ is defined below as a fixed variable\n",
    "    * Distance Threshold = Small distance in meters\n",
    "        * $d_{\\textrm{thresh}} = 20 meters$\n",
    "        * This is the distance between a possible point and the current point for consideration.\n",
    "    * Step Distance = Small distance in meters.\n",
    "        * $d_{\\textrm{step}} = 20 meters$\n",
    "        * This is the distance that the route will jump using the average angle\n",
    "    * Waypoint list = empty array\n",
    "        * $\\hat{P}_{\\textrm{waypoints}} = []$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = 'bike_data.db'\n",
    "\n",
    "start_coord = (39.5989743, -104.8609468)\n",
    "end_coord   = (39.75428108249532, -105.00085402872664)\n",
    "\n",
    "epsg_code = 32613"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pyproj import CRS, Proj, Transformer\n",
    "from prinpy.local import CLPCG\n",
    "from prinpy.glob import NLPCA\n",
    "import re, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Setup the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLAlchemy connectable \n",
    "conn = create_engine( 'sqlite:///' + database_path ).connect()\n",
    "\n",
    "#  For each segment, we need to create a track for each dataset\n",
    "dataset_ids = pd.read_sql_query('SELECT DISTINCT datasetId FROM point_list', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Setup UTM Coordinate Projection API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTM Grid Zone: 13\n"
     ]
    }
   ],
   "source": [
    "#  Setup the Projection Transformer\n",
    "crs = CRS.from_epsg( epsg_code )\n",
    "proj_dd2utm = Transformer.from_crs(crs.geodetic_crs, crs)\n",
    "proj_utm2dd = Transformer.from_crs(crs, crs.geodetic_crs)\n",
    "utm_zone = int(re.findall(\"\\d+\", crs.utm_zone)[0])\n",
    "print('UTM Grid Zone: {}'.format(utm_zone))\n",
    "\n",
    "(easting,northing) = proj_dd2utm.transform( start_coord[0], start_coord[1] )\n",
    "start_coord_utm = np.array( [easting, northing], np.float64 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Routes for Each Sector\n",
    "\n",
    "This algorithm works best with small batches of points.  It's important to work sector-by-sector to build the final route. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Sector( sector_id, start_point_utm, e_max ):\n",
    "    \n",
    "    #  Load coordinates\n",
    "    sql_query = 'SELECT * FROM point_list WHERE sector_id is {} ORDER BY timestamp'.format( sector_id )\n",
    "    points = pd.read_sql_query( sql_query, conn )\n",
    "    utm_points = points.loc[:,['easting','northing']]\n",
    "    print(sql_query)\n",
    "    \n",
    "    #  Create solver\n",
    "    cl = CLPCG()\n",
    "    \n",
    "    # the fit() method calculates the principal curve\n",
    "    # e_max is determined through trial and error as of\n",
    "    # now, but aim for about 1/2 data error and adjust from\n",
    "    # there.\n",
    "    x_vals = utm_points.loc[:,'easting']\n",
    "    y_vals = utm_points.loc[:,'northing']\n",
    "\n",
    "    x_vals = pd.concat([pd.Series([easting]), x_vals])\n",
    "    y_vals = pd.concat([pd.Series([northing]), y_vals])\n",
    "\n",
    "    x_min = x_vals.min()\n",
    "    x_max = x_vals.max()\n",
    "    y_min = y_vals.min()\n",
    "    y_max = y_vals.max()\n",
    "\n",
    "    x_vals_norm = 2 * (x_vals - x_min) / (x_max - x_min) - 1\n",
    "    y_vals_norm = 2 * (y_vals - y_min) / (y_max - y_min) - 1\n",
    "\n",
    "    cl.fit( x_vals_norm.to_numpy(), \n",
    "            y_vals_norm.to_numpy(),\n",
    "            e_max = 0.05 )  # CLPCG.fit() to fit PC\n",
    "\n",
    "    spline_pts_local = cl.fit_points   # fitted points with PC that spline is passed through\n",
    "    spline_x_local = ( (spline_pts_local[:,0] + 1.0)/2.0 ) * ( x_max - x_min ) + x_min\n",
    "    spline_y_local = ( (spline_pts_local[:,1] + 1.0)/2.0 ) * ( y_max - y_min ) + y_min\n",
    "\n",
    "    # Convert Spline Points to Lat/Lon\n",
    "    prin_lats_local, prin_lons_local = proj_utm2dd.transform( spline_x_local, spline_y_local )\n",
    "    print('Spline has {} coordinates'.format(len(spline_x_local)))\n",
    "    \n",
    "    return spline_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Iterate through each sector, constructing the path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:  Compute the remaining route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'points_by_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e27960709bd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#  Update Current Indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datasetId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         point_dd  = [ points_by_dataset[dataset_id]['points']['latitude'][current_dataset_idx[dataset_id]],\n\u001b[0m\u001b[1;32m      7\u001b[0m                       points_by_dataset[dataset_id]['points']['longitude'][current_dataset_idx[dataset_id]] ]\n\u001b[1;32m      8\u001b[0m         point_utm = np.array( [ points_by_dataset[dataset_id]['points']['easting'][current_dataset_idx[dataset_id]],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'points_by_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,600):\n",
    "    temp_angles = []\n",
    "    \n",
    "    #  Update Current Indices\n",
    "    for dataset_id in dataset_ids['datasetId']:\n",
    "        point_dd  = [ points_by_dataset[dataset_id]['points']['latitude'][current_dataset_idx[dataset_id]],\n",
    "                      points_by_dataset[dataset_id]['points']['longitude'][current_dataset_idx[dataset_id]] ]\n",
    "        point_utm = np.array( [ points_by_dataset[dataset_id]['points']['easting'][current_dataset_idx[dataset_id]],\n",
    "                                points_by_dataset[dataset_id]['points']['northing'][current_dataset_idx[dataset_id]] ], np.float64 )\n",
    "        \n",
    "        #  Check distance from starting point\n",
    "        geod = Geodesic.WGS84.Inverse( waypoint_list[-1][0][0], \n",
    "                                       waypoint_list[-1][0][1],\n",
    "                                       point_dd[0],\n",
    "                                       point_dd[1] )\n",
    "        dist = np.linalg.norm( point_utm - waypoint_list[-1][1] )\n",
    "    \n",
    "        if dist < dist_thresh_m:\n",
    "        \n",
    "                #  Compute angle to new point\n",
    "                temp_angles.append( geod['azi1'])\n",
    "\n",
    "    #  Compute seed point\n",
    "    avg_angle = mean_angle( temp_angles )\n",
    "    next_point_dd = Geodesic.WGS84.Direct( waypoint_list[-1][0][0], \n",
    "                                           waypoint_list[-1][0][1],\n",
    "                                           avg_angle, \n",
    "                                           step_dist_m )\n",
    "    next_point_utm = proj_dd2utm.transform( next_point_dd['lat2'], next_point_dd['lon2'] )\n",
    "\n",
    "    waypoint_list.append( [ np.array( [ next_point_dd['lat2'], next_point_dd['lon2'] ] ), \n",
    "                            np.array( [ next_point_utm[0], next_point_utm[1] ], np.float64 ),\n",
    "                            avg_angle ] )\n",
    "    \n",
    "    #  Update our iterators past all points \"behind\" the coordinate\n",
    "    for dataset_id in dataset_ids['datasetId']:\n",
    "\n",
    "        while True:\n",
    "            point_dd  = np.array( [ points_by_dataset[dataset_id]['points']['latitude'][current_dataset_idx[dataset_id]],\n",
    "                                    points_by_dataset[dataset_id]['points']['longitude'][current_dataset_idx[dataset_id]] ], np.float64 )\n",
    "            point_utm = np.array( [ points_by_dataset[dataset_id]['points']['easting'][current_dataset_idx[dataset_id]],\n",
    "                                    points_by_dataset[dataset_id]['points']['northing'][current_dataset_idx[dataset_id]] ], np.float64 )\n",
    "        \n",
    "            # Update the indeces until the next coordinate is no longer in \"front\"\n",
    "            if ( not Is_In_Front( waypoint_list[-1][0], point_dd, waypoint_list[-1][2] ) ) and ( np.linalg.norm( point_utm - waypoint_list[-1][1] ) < step_dist_m ):\n",
    "                current_dataset_idx[dataset_id] += 1\n",
    "            else:\n",
    "                break  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_pt = [0, 0]\n",
    "\n",
    "#  Create the polyline and list of points\n",
    "marker_list = [[start_coord[0],start_coord[1]]]\n",
    "polyline   = []\n",
    "\n",
    "for point in waypoint_list:\n",
    "    marker_list.append( [point[0][0],point[0][1]] )\n",
    "    centroid_pt[0] += point[0][0]\n",
    "    centroid_pt[1] += point[0][1]\n",
    "centroid_pt[0] /= len( waypoint_list )\n",
    "centroid_pt[1] /= len( waypoint_list )\n",
    "print( centroid_pt )\n",
    "\n",
    "#  Build Map Visualization    \n",
    "sector_map = Map( center=centroid_pt, zoom=14 )\n",
    "\n",
    "#---------------------------------#\n",
    "#-      Print the datasets       -#\n",
    "#---------------------------------#\n",
    "dataset_points = []\n",
    "dataset_counter = 0.0\n",
    "for dataset_id in dataset_ids['datasetId']:\n",
    "    dataset_route = []\n",
    "    for pidx in range( 0, len( points_by_dataset[dataset_id]['points']['latitude'] ) ):\n",
    "        test_point = [ points_by_dataset[dataset_id]['points']['latitude'][pidx],\n",
    "                       points_by_dataset[dataset_id]['points']['longitude'][pidx] ]\n",
    "        #dataset_points.append( Marker( location=test_point, \n",
    "        #                               draggable=False,\n",
    "        #                               color=matplotlib.colors.rgb2hex( plt.get_cmap('hsv')( dataset_counter / len(dataset_ids['datasetId']) ) ) ) )\n",
    "        dataset_route.append( test_point )\n",
    "    sector_map.add_layer( Polyline( locations= dataset_route,\n",
    "                                    color=matplotlib.colors.rgb2hex( plt.get_cmap('hsv')( dataset_counter / len(dataset_ids['datasetId']) ) ),\n",
    "                                    fill=False ) )\n",
    "    \n",
    "for p in dataset_points:\n",
    "    sector_map.add_layer( p )\n",
    "    \n",
    "#-------------------------------------#\n",
    "#-      Print the average route      -#\n",
    "#-------------------------------------#\n",
    "sector_map.add_layer( Marker( location=[start_coord[0],start_coord[1]] ) )\n",
    "#for marker in marker_list:\n",
    "#    marker = Marker( location=marker, draggable=False )\n",
    "#    sector_map.add_layer( marker )\n",
    "   \n",
    "route_poly = Polyline( locations=marker_list,\n",
    "                       color='blue',\n",
    "                       fill=False )\n",
    "sector_map.add_layer( route_poly )\n",
    "    \n",
    "#sector_map.layout.height=\"400px\"\n",
    "sector_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
